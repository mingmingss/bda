"""
KBO ì •ê·œì‹œì¦Œ ê´€ì¤‘ ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• - ê°œì„  ë²„ì „ v2
3ì¡° - ëª¨ë¸ êµ¬ì¶• ë‹´ë‹¹: ì„í˜œë¦°, ìœ¤íƒœì˜

ê°œì„  ì‚¬í•­ v2:
1. âœ… ë°ì´í„° ëˆ„ì¶œ(Data Leakage) ì™„ì „ ì œê±° - ì‹œê³„ì—´ ìˆœì„œë¥¼ ì—„ê²©íˆ ë³´ì¡´
2. âœ… TimeSeriesSplit êµì°¨ ê²€ì¦ ë„ì…
3. âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¶”ê°€ (RandomizedSearchCV)
4. âœ… ì½”ë“œ êµ¬ì¡°í™” - í•¨ìˆ˜/ëª¨ë“ˆí™”
5. âœ… ì¬ì‚¬ìš©ì„± ë° í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í–¥ìƒ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.ensemble import (
    RandomForestRegressor,
    HistGradientBoostingRegressor,
    AdaBoostRegressor,
    ExtraTreesRegressor,
    VotingRegressor
)
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
from sklearn.base import clone
import warnings
from pathlib import Path
import joblib
from typing import Dict, List, Tuple, Any
warnings.filterwarnings('ignore')

# XGBoost, CatBoost, LightGBM import (ì„ íƒì )
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸  XGBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install xgboost")

try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("âš ï¸  CatBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install catboost")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸  LightGBMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install lightgbm")

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
sns.set_style('whitegrid')


# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================

def load_data(data_path: Path) -> pd.DataFrame:
    """
    ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê¸°ë³¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        data_path: CSV íŒŒì¼ ê²½ë¡œ

    Returns:
        ì „ì²˜ë¦¬ëœ DataFrame
    """
    print("\n[Phase 1] ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰")
    print("-" * 80)

    df = pd.read_csv(data_path, encoding='utf-8-sig')
    print(f"ë°ì´í„° shape: {df.shape}")
    print(f"\nê²°ì¸¡ì¹˜ í˜„í™©:\n{df.isnull().sum()}")

    # ê¸°ë³¸ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    missing_cols = df.columns[df.isnull().any()].tolist()
    if missing_cols:
        print(f"\nê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼: {missing_cols}")
        for col in missing_cols:
            if df[col].dtype in ['float64', 'int64']:
                mean_value = df[col].mean()
                df[col].fillna(mean_value, inplace=True)
                print(f"  - {col}: í‰ê· ê°’({mean_value:.2f})ìœ¼ë¡œ ëŒ€ì²´")

    # ì¼ì‹œ ë³€í™˜
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Yë…„%mì›”%dì¼')
    df = df.sort_values('ì¼ì‹œ').reset_index(drop=True)
    print(f"\nì¼ì‹œ ë²”ìœ„: {df['ì¼ì‹œ'].min()} ~ {df['ì¼ì‹œ'].max()}")

    # ì‹œê°„ ì»¬ëŸ¼ ì¶”ì¶œ
    df['ì—°ë„'] = df['ì¼ì‹œ'].dt.year
    df['ì›”'] = df['ì¼ì‹œ'].dt.month
    df['ì¼'] = df['ì¼ì‹œ'].dt.day
    df['ì£¼ì°¨'] = df['ì¼ì‹œ'].dt.isocalendar().week

    print(f"\nì—°ë„ë³„ ê²½ê¸° ìˆ˜:\n{df['ì—°ë„'].value_counts().sort_index()}")

    return df


def engineer_basic_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì‹œê°„, ë‚ ì”¨, êµ¬ì¥ ë“± ê¸°ë³¸ featuresë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    (íŒ€ ì¸ê¸°ë„ëŠ” ë³„ë„ í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬ - ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)

    Args:
        df: ì›ë³¸ DataFrame

    Returns:
        Featureê°€ ì¶”ê°€ëœ DataFrame
    """
    print("\n[Phase 2] ê¸°ë³¸ Feature Engineering")
    print("-" * 80)

    df = df.copy()

    # 1. ì‹œê°„ ê´€ë ¨ Features
    print("1. ì‹œê°„ ê´€ë ¨ Features ìƒì„±...")
    weekday_map = {
        'ì›”ìš”ì¼': 0, 'í™”ìš”ì¼': 1, 'ìˆ˜ìš”ì¼': 2, 'ëª©ìš”ì¼': 3,
        'ê¸ˆìš”ì¼': 4, 'í† ìš”ì¼': 5, 'ì¼ìš”ì¼': 6
    }
    df['ìš”ì¼_ìˆ«ì'] = df['ìš”ì¼'].map(weekday_map)

    # NaN ì²˜ë¦¬ - ìš”ì¼ì´ ì—†ìœ¼ë©´ ì¼ì‹œì—ì„œ ì¶”ì¶œ
    if df['ìš”ì¼_ìˆ«ì'].isnull().any():
        df.loc[df['ìš”ì¼_ìˆ«ì'].isnull(), 'ìš”ì¼_ìˆ«ì'] = df.loc[df['ìš”ì¼_ìˆ«ì'].isnull(), 'ì¼ì‹œ'].dt.dayofweek

    df['ì£¼ë§ì—¬ë¶€'] = df['ìš”ì¼_ìˆ«ì'].apply(lambda x: 1 if x >= 4 else 0)
    df['ì›”_sin'] = np.sin(2 * np.pi * df['ì›”'] / 12)
    df['ì›”_cos'] = np.cos(2 * np.pi * df['ì›”'] / 12)

    # 2. ë‚ ì”¨ ê´€ë ¨ Features
    print("2. ë‚ ì”¨ ê´€ë ¨ Features ìƒì„±...")
    df['í‰ê· ê¸°ì˜¨'] = (df['ìµœì €ê¸°ì˜¨(Â°C)'] + df['ìµœê³ ê¸°ì˜¨(Â°C)']) / 2
    df['ë¹„ì—¬ë¶€'] = (df['ì¼ê°•ìˆ˜ëŸ‰(mm)'] > 0).astype(int)

    def categorize_rain(rain):
        if rain == 0:
            return 0
        elif rain < 5:
            return 1
        elif rain < 20:
            return 2
        else:
            return 3

    df['ê°•ìˆ˜ê°•ë„'] = df['ì¼ê°•ìˆ˜ëŸ‰(mm)'].apply(categorize_rain)

    def categorize_weather(cloud):
        if pd.isna(cloud):
            return 1  # ì¤‘ê°„ê°’ìœ¼ë¡œ ëŒ€ì²´
        if cloud <= 3:
            return 0
        elif cloud <= 7:
            return 1
        else:
            return 2

    df['ë‚ ì”¨ìƒíƒœ'] = df['í‰ê·  ì „ìš´ëŸ‰(1/10)'].apply(categorize_weather)

    # 3. êµ¬ì¥ ê´€ë ¨ Features
    print("3. êµ¬ì¥ ê´€ë ¨ Features ìƒì„±...")
    stadium_capacity = {
        'ì ì‹¤': 25000, 'ìˆ˜ì›': 20000, 'ë¬¸í•™': 20500, 'ê³ ì²™': 16500,
        'ê´‘ì£¼': 11000, 'ì‚¬ì§': 23000, 'ì°½ì›': 20000, 'ëŒ€êµ¬': 10000,
        'í•œë°­': 11500, 'ëŒ€ì „': 13000, 'ìš¸ì‚°': 11000, 'í¬í•­': 11000, 'ì²­ì£¼': 10000
    }

    df['êµ¬ì¥ìˆ˜ìš©ì¸ì›'] = df['êµ¬ì¥'].map(stadium_capacity)
    df['êµ¬ì¥í¬ê¸°'] = pd.cut(df['êµ¬ì¥ìˆ˜ìš©ì¸ì›'],
                          bins=[0, 12000, 20000, 30000],
                          labels=[0, 1, 2]).astype(float)

    # 4. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
    print("4. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©...")
    le_stadium = LabelEncoder()
    le_home = LabelEncoder()
    le_away = LabelEncoder()

    df['êµ¬ì¥_ì¸ì½”ë”©'] = le_stadium.fit_transform(df['êµ¬ì¥'])
    df['í™ˆíŒ€_ì¸ì½”ë”©'] = le_home.fit_transform(df['í™ˆ'])
    df['ë°©ë¬¸íŒ€_ì¸ì½”ë”©'] = le_away.fit_transform(df['ë°©ë¬¸'])

    # 5. ë§¤ì§„ìœ¨ (ì°¸ê³ ìš©)
    df['ë§¤ì§„ìœ¨'] = df['ê´€ì¤‘ìˆ˜'] / df['êµ¬ì¥ìˆ˜ìš©ì¸ì›']

    print("âœ“ ê¸°ë³¸ Feature Engineering ì™„ë£Œ")

    return df


def calculate_popularity_features_no_leakage(df: pd.DataFrame,
                                               train_end_date: pd.Timestamp) -> pd.DataFrame:
    """
    ë°ì´í„° ëˆ„ì¶œ ì—†ì´ íŒ€ ì¸ê¸°ë„ featuresë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    í•µì‹¬: train_end_date ì´ì „ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ í†µê³„ëŸ‰ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        df: ì „ì²´ DataFrame
        train_end_date: í•™ìŠµ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ

    Returns:
        ì¸ê¸°ë„ featuresê°€ ì¶”ê°€ëœ DataFrame
    """
    df = df.copy()

    # train_end_date ì´ì „ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
    train_mask = df['ì¼ì‹œ'] <= train_end_date
    train_data = df[train_mask]

    if len(train_data) == 0:
        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì „ì²´ í‰ê·  ì‚¬ìš©
        overall_avg = df['ê´€ì¤‘ìˆ˜'].mean()
    else:
        overall_avg = train_data['ê´€ì¤‘ìˆ˜'].mean()

    # í™ˆíŒ€ ì¸ê¸°ë„
    home_popularity = train_data.groupby('í™ˆ')['ê´€ì¤‘ìˆ˜'].mean().to_dict()
    df['í™ˆíŒ€í‰ê· ê´€ì¤‘'] = df['í™ˆ'].map(home_popularity)
    df['í™ˆíŒ€í‰ê· ê´€ì¤‘'].fillna(overall_avg, inplace=True)

    # ë°©ë¬¸íŒ€ ì¸ê¸°ë„
    away_popularity = train_data.groupby('ë°©ë¬¸')['ê´€ì¤‘ìˆ˜'].mean().to_dict()
    df['ë°©ë¬¸íŒ€í‰ê· ê´€ì¤‘'] = df['ë°©ë¬¸'].map(away_popularity)
    df['ë°©ë¬¸íŒ€í‰ê· ê´€ì¤‘'].fillna(overall_avg, inplace=True)

    # ì¸ê¸° ì§€ìˆ˜
    df['í™ˆíŒ€ì¸ê¸°ì§€ìˆ˜'] = df['í™ˆíŒ€í‰ê· ê´€ì¤‘'] / overall_avg
    df['ë°©ë¬¸íŒ€ì¸ê¸°ì§€ìˆ˜'] = df['ë°©ë¬¸íŒ€í‰ê· ê´€ì¤‘'] / overall_avg

    # ëŒ€ì§„ ì¸ê¸°ë„
    matchup = train_data.groupby(['í™ˆ', 'ë°©ë¬¸'])['ê´€ì¤‘ìˆ˜'].mean().to_dict()
    df['ëŒ€ì§„í‰ê· ê´€ì¤‘'] = df.apply(lambda row: matchup.get((row['í™ˆ'], row['ë°©ë¬¸']), overall_avg), axis=1)
    df['ëŒ€ì§„ì¸ê¸°ì§€ìˆ˜'] = df['ëŒ€ì§„í‰ê· ê´€ì¤‘'] / overall_avg

    return df


def calculate_moving_average_and_target(df: pd.DataFrame) -> pd.DataFrame:
    """
    ì´ë™í‰ê·  ë° ëª©í‘œ ë³€ìˆ˜(ê´€ì¤‘ë¹„ìœ¨)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        df: DataFrame

    Returns:
        ì´ë™í‰ê· ê³¼ ëª©í‘œë³€ìˆ˜ê°€ ì¶”ê°€ëœ DataFrame
    """
    df = df.copy()

    print("\nì´ë™í‰ê·  ë° ëª©í‘œ ë³€ìˆ˜ ê³„ì‚°...")
    df['ì´ë™í‰ê· _30'] = df['ê´€ì¤‘ìˆ˜'].rolling(window=30, min_periods=1).mean().shift(1)

    # ì´ˆê¸°ê°’ ì²˜ë¦¬
    initial_avg = df.iloc[:30]['ê´€ì¤‘ìˆ˜'].mean()
    df.loc[df['ì´ë™í‰ê· _30'].isna(), 'ì´ë™í‰ê· _30'] = initial_avg

    # ëª©í‘œ ë³€ìˆ˜: ê´€ì¤‘ ë¹„ìœ¨
    df['ê´€ì¤‘ë¹„ìœ¨'] = df['ê´€ì¤‘ìˆ˜'] / df['ì´ë™í‰ê· _30']

    print(f"  - ê´€ì¤‘ë¹„ìœ¨ í‰ê· : {df['ê´€ì¤‘ë¹„ìœ¨'].mean():.3f}")
    print(f"  - ê´€ì¤‘ë¹„ìœ¨ ë²”ìœ„: {df['ê´€ì¤‘ë¹„ìœ¨'].min():.3f} ~ {df['ê´€ì¤‘ë¹„ìœ¨'].max():.3f}")

    return df


def handle_final_nans(df: pd.DataFrame, feature_columns: List[str]) -> pd.DataFrame:
    """
    ìµœì¢… NaN ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Args:
        df: DataFrame
        feature_columns: í™•ì¸í•  feature ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸

    Returns:
        NaNì´ ì œê±°ëœ DataFrame
    """
    df = df.copy()

    print("\nìµœì¢… NaN ì²´í¬ ë° ì²˜ë¦¬...")

    for col in feature_columns:
        if col not in df.columns:
            continue

        nan_count = df[col].isnull().sum()
        if nan_count > 0:
            print(f"âš ï¸  {col}: {nan_count}ê°œ NaN ë°œê²¬")

            if df[col].dtype in ['float64', 'int64']:
                mean_val = df[col].mean()
                if pd.isna(mean_val):
                    df[col].fillna(0, inplace=True)
                    print(f"    â†’ 0ìœ¼ë¡œ ëŒ€ì²´")
                else:
                    df[col].fillna(mean_val, inplace=True)
                    print(f"    â†’ í‰ê· ê°’({mean_val:.2f})ìœ¼ë¡œ ëŒ€ì²´")
            else:
                mode_val = df[col].mode()[0] if len(df[col].mode()) > 0 else 0
                df[col].fillna(mode_val, inplace=True)
                print(f"    â†’ ìµœë¹ˆê°’({mode_val})ìœ¼ë¡œ ëŒ€ì²´")

    print("âœ“ NaN ì²˜ë¦¬ ì™„ë£Œ")
    return df


# ============================================================================
# 2. ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ í•¨ìˆ˜
# ============================================================================

def get_model_definitions() -> Dict[str, Any]:
    """
    ì‚¬ìš©í•  ëª¨ë¸ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.

    Returns:
        ëª¨ë¸ ì´ë¦„ê³¼ ëª¨ë¸ ê°ì²´ì˜ ë”•ì…”ë„ˆë¦¬
    """
    models = {}

    models['Decision Tree'] = DecisionTreeRegressor(random_state=42, max_depth=10)

    models['Random Forest'] = RandomForestRegressor(
        n_estimators=200,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        n_jobs=-1
    )

    models['Extra Trees'] = ExtraTreesRegressor(
        n_estimators=200,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        n_jobs=-1
    )

    models['Hist Gradient Boosting'] = HistGradientBoostingRegressor(
        max_iter=200,
        max_depth=5,
        learning_rate=0.1,
        random_state=42
    )

    models['AdaBoost'] = AdaBoostRegressor(
        estimator=DecisionTreeRegressor(max_depth=5),
        n_estimators=100,
        learning_rate=0.1,
        random_state=42
    )

    if XGBOOST_AVAILABLE:
        models['XGBoost'] = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=5,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )

    if CATBOOST_AVAILABLE:
        models['CatBoost'] = cb.CatBoostRegressor(
            iterations=200,
            depth=5,
            learning_rate=0.1,
            random_state=42,
            verbose=0
        )

    if LIGHTGBM_AVAILABLE:
        models['LightGBM'] = lgb.LGBMRegressor(
            n_estimators=200,
            max_depth=5,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1,
            verbose=-1
        )

    return models


def get_hyperparameter_grids() -> Dict[str, Dict]:
    """
    í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

    Returns:
        ëª¨ë¸ ì´ë¦„ê³¼ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œì˜ ë”•ì…”ë„ˆë¦¬
    """
    param_grids = {}

    # Random Forest
    param_grids['Random Forest'] = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 15, 20, None],
        'min_samples_split': [5, 10, 20],
        'min_samples_leaf': [2, 5, 10],
        'max_features': ['sqrt', 'log2', None]
    }

    # Extra Trees
    param_grids['Extra Trees'] = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 15, 20, None],
        'min_samples_split': [5, 10, 20],
        'min_samples_leaf': [2, 5, 10],
        'max_features': ['sqrt', 'log2', None]
    }

    # Hist Gradient Boosting
    param_grids['Hist Gradient Boosting'] = {
        'max_iter': [100, 200, 300],
        'max_depth': [3, 5, 7, 10],
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'min_samples_leaf': [10, 20, 30]
    }

    if XGBOOST_AVAILABLE:
        param_grids['XGBoost'] = {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 5, 7, 10],
            'learning_rate': [0.01, 0.05, 0.1, 0.2],
            'subsample': [0.7, 0.8, 0.9, 1.0],
            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
            'min_child_weight': [1, 3, 5]
        }

    if LIGHTGBM_AVAILABLE:
        param_grids['LightGBM'] = {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 5, 7, 10, -1],
            'learning_rate': [0.01, 0.05, 0.1, 0.2],
            'subsample': [0.7, 0.8, 0.9, 1.0],
            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
            'num_leaves': [20, 31, 50, 100]
        }

    return param_grids


def train_with_timeseries_cv(df: pd.DataFrame,
                             feature_columns: List[str],
                             train_years: List[int],
                             val_years: List[int],
                             n_splits: int = 3) -> Tuple[Dict, pd.DataFrame]:
    """
    TimeSeriesSplitì„ ì‚¬ìš©í•œ êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

    Args:
        df: ì „ì²´ DataFrame
        feature_columns: ì‚¬ìš©í•  feature ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        train_years: í•™ìŠµì— ì‚¬ìš©í•  ì—°ë„ ë¦¬ìŠ¤íŠ¸
        val_years: ê²€ì¦ì— ì‚¬ìš©í•  ì—°ë„ ë¦¬ìŠ¤íŠ¸
        n_splits: TimeSeriesSplitì˜ fold ìˆ˜

    Returns:
        (í•™ìŠµëœ ëª¨ë¸ ë”•ì…”ë„ˆë¦¬, ê²°ê³¼ DataFrame)
    """
    print("\n[Phase 3] TimeSeriesSplit êµì°¨ ê²€ì¦ì„ í†µí•œ ëª¨ë¸ í•™ìŠµ")
    print("-" * 80)

    # ë°ì´í„° ë¶„í• 
    train_data = df[df['ì—°ë„'].isin(train_years)].copy()
    val_data = df[df['ì—°ë„'].isin(val_years)].copy()

    print(f"\nTrain years: {train_years} ({len(train_data)}ê²½ê¸°)")
    print(f"Val years: {val_years} ({len(val_data)}ê²½ê¸°)")

    X_train = train_data[feature_columns]
    y_train = train_data['ê´€ì¤‘ë¹„ìœ¨']

    X_val = val_data[feature_columns]
    y_val = val_data['ê´€ì¤‘ë¹„ìœ¨']

    # TimeSeriesSplit ì„¤ì •
    tscv = TimeSeriesSplit(n_splits=n_splits)

    # ëª¨ë¸ ì •ì˜
    models = get_model_definitions()

    results = []
    trained_models = {}

    print(f"\nTimeSeriesSplit ({n_splits} folds) êµì°¨ ê²€ì¦ ì‹œì‘...")
    print("=" * 80)

    for name, model in models.items():
        print(f"\n[{name}] í•™ìŠµ ì¤‘...")

        # TimeSeriesSplit CV ì ìˆ˜
        cv_scores = []
        for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X_train)):
            X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

            # ëª¨ë¸ í•™ìŠµ (cloneì„ ì‚¬ìš©í•˜ì—¬ nested estimator ë¬¸ì œ í•´ê²°)
            model_fold = clone(model)
            model_fold.fit(X_fold_train, y_fold_train)

            # ê²€ì¦
            y_fold_pred = model_fold.predict(X_fold_val)
            fold_mae = mean_absolute_error(y_fold_val, y_fold_pred)
            cv_scores.append(fold_mae)

            print(f"  Fold {fold_idx+1}/{n_splits}: MAE = {fold_mae:.4f}")

        cv_mean = np.mean(cv_scores)
        cv_std = np.std(cv_scores)

        print(f"  CV MAE: {cv_mean:.4f} (+/- {cv_std:.4f})")

        # ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ ìµœì¢… í•™ìŠµ
        model.fit(X_train, y_train)
        trained_models[name] = model

        # ì˜ˆì¸¡
        y_train_pred = model.predict(X_train)
        y_val_pred = model.predict(X_val)

        # í‰ê°€
        train_mae = mean_absolute_error(y_train, y_train_pred)
        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
        train_r2 = r2_score(y_train, y_train_pred)

        val_mae = mean_absolute_error(y_val, y_val_pred)
        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
        val_r2 = r2_score(y_val, y_val_pred)

        # ê²°ê³¼ ì €ì¥
        results.append({
            'Model': name,
            'CV MAE Mean': cv_mean,
            'CV MAE Std': cv_std,
            'Train MAE': train_mae,
            'Train RMSE': train_rmse,
            'Train RÂ²': train_r2,
            'Val MAE': val_mae,
            'Val RMSE': val_rmse,
            'Val RÂ²': val_r2,
            'Overfit Score': train_mae - val_mae
        })

        print(f"  Train - MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}, RÂ²: {train_r2:.4f}")
        print(f"  Val   - MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, RÂ²: {val_r2:.4f}")

    # ê²°ê³¼ DataFrame
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('Val MAE')

    print("\n" + "=" * 80)
    print("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (Validation MAE ê¸°ì¤€ ì •ë ¬)")
    print("=" * 80)
    print(results_df.to_string(index=False))

    return trained_models, results_df


def tune_hyperparameters(X_train: pd.DataFrame,
                         y_train: pd.Series,
                         model_name: str,
                         base_model: Any,
                         param_grid: Dict,
                         n_iter: int = 20,
                         n_splits: int = 3) -> Tuple[Any, Dict]:
    """
    RandomizedSearchCVë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•©ë‹ˆë‹¤.

    Args:
        X_train: í•™ìŠµ features
        y_train: í•™ìŠµ target
        model_name: ëª¨ë¸ ì´ë¦„
        base_model: ê¸°ë³¸ ëª¨ë¸
        param_grid: íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
        n_iter: RandomizedSearch ë°˜ë³µ íšŸìˆ˜
        n_splits: TimeSeriesSplit fold ìˆ˜

    Returns:
        (ìµœì  ëª¨ë¸, ìµœì  íŒŒë¼ë¯¸í„°)
    """
    print(f"\n[{model_name}] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘...")
    print(f"  - íƒìƒ‰ ê³µê°„: {len(param_grid)} íŒŒë¼ë¯¸í„°")
    print(f"  - ë°˜ë³µ íšŸìˆ˜: {n_iter}")
    print(f"  - CV folds: {n_splits}")

    tscv = TimeSeriesSplit(n_splits=n_splits)

    random_search = RandomizedSearchCV(
        estimator=base_model,
        param_distributions=param_grid,
        n_iter=n_iter,
        cv=tscv,
        scoring='neg_mean_absolute_error',
        n_jobs=-1,
        random_state=42,
        verbose=0
    )

    random_search.fit(X_train, y_train)

    best_params = random_search.best_params_
    best_score = -random_search.best_score_

    print(f"  âœ“ ìµœì  CV MAE: {best_score:.4f}")
    print(f"  âœ“ ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")

    return random_search.best_estimator_, best_params


# ============================================================================
# 3. í‰ê°€ ë° ì €ì¥ í•¨ìˆ˜
# ============================================================================

def evaluate_on_test(model: Any,
                     test_data: pd.DataFrame,
                     feature_columns: List[str],
                     baseline_2025: float) -> Dict:
    """
    í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.

    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        test_data: í…ŒìŠ¤íŠ¸ DataFrame
        feature_columns: feature ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
        baseline_2025: 2025ë…„ baseline ê°’

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    X_test = test_data[feature_columns]
    y_test = test_data['ê´€ì¤‘ë¹„ìœ¨']

    # ë¹„ìœ¨ ì˜ˆì¸¡
    y_test_pred_ratio = model.predict(X_test)

    # ê´€ì¤‘ìˆ˜ ë³€í™˜
    predicted_attendance = y_test_pred_ratio * baseline_2025
    actual_attendance = test_data['ê´€ì¤‘ìˆ˜'].values

    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    # ë¹„ìœ¨ ê¸°ì¤€
    mae_ratio = mean_absolute_error(y_test, y_test_pred_ratio)
    rmse_ratio = np.sqrt(mean_squared_error(y_test, y_test_pred_ratio))
    r2_ratio = r2_score(y_test, y_test_pred_ratio)
    mape_ratio = np.mean(np.abs((y_test - y_test_pred_ratio) / y_test)) * 100

    # ê´€ì¤‘ìˆ˜ ê¸°ì¤€
    mae_attendance = mean_absolute_error(actual_attendance, predicted_attendance)
    rmse_attendance = np.sqrt(mean_squared_error(actual_attendance, predicted_attendance))
    r2_attendance = r2_score(actual_attendance, predicted_attendance)
    mape_attendance = np.mean(np.abs((actual_attendance - predicted_attendance) / actual_attendance)) * 100

    return {
        'mae_ratio': mae_ratio,
        'rmse_ratio': rmse_ratio,
        'r2_ratio': r2_ratio,
        'mape_ratio': mape_ratio,
        'mae_attendance': mae_attendance,
        'rmse_attendance': rmse_attendance,
        'r2_attendance': r2_attendance,
        'mape_attendance': mape_attendance,
        'predicted_attendance': predicted_attendance,
        'actual_attendance': actual_attendance
    }


def save_results(output_dir: Path,
                 results_df: pd.DataFrame,
                 test_results: pd.DataFrame,
                 feature_importance_df: pd.DataFrame,
                 best_model: Any,
                 best_model_name: str,
                 tuned_params: Dict = None):
    """
    ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        results_df: ëª¨ë¸ ë¹„êµ ê²°ê³¼
        test_results: í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ê²°ê³¼
        feature_importance_df: Feature importance
        best_model: ìµœì  ëª¨ë¸
        best_model_name: ìµœì  ëª¨ë¸ ì´ë¦„
        tuned_params: íŠœë‹ëœ íŒŒë¼ë¯¸í„° (ì„ íƒì )
    """
    print("\n[Phase Final] ê²°ê³¼ ì €ì¥")
    print("-" * 80)

    output_dir.mkdir(exist_ok=True)

    # 1. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ
    results_df.to_csv(output_dir / "model_performance_comparison_v2.csv",
                     index=False, encoding='utf-8-sig')
    print(f"âœ“ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ ì €ì¥: model_performance_comparison_v2.csv")

    # 2. ì˜ˆì¸¡ ê²°ê³¼
    test_results.to_csv(output_dir / "2025_predictions_detailed_v2.csv",
                       index=False, encoding='utf-8-sig')
    print(f"âœ“ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: 2025_predictions_detailed_v2.csv")

    # 3. Feature Importance
    if feature_importance_df is not None:
        feature_importance_df.to_csv(output_dir / "feature_importance_v2.csv",
                                    index=False, encoding='utf-8-sig')
        print(f"âœ“ Feature Importance ì €ì¥: feature_importance_v2.csv")

    # 4. íŠœë‹ëœ íŒŒë¼ë¯¸í„°
    if tuned_params:
        params_df = pd.DataFrame([tuned_params])
        params_df.to_csv(output_dir / "best_hyperparameters_v2.csv",
                        index=False, encoding='utf-8-sig')
        print(f"âœ“ ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥: best_hyperparameters_v2.csv")

    # 5. ìµœì¢… ëª¨ë¸
    model_path = output_dir / f"best_model_{best_model_name.replace(' ', '_')}_v2.pkl"
    joblib.dump(best_model, model_path)
    print(f"âœ“ ìµœì¢… ëª¨ë¸ ì €ì¥: {model_path.name}")

    print("\nâœ“ ëª¨ë“  ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")


# ============================================================================
# 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("=" * 80)
    print("KBO ê´€ì¤‘ ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• í”„ë¡œì íŠ¸ - ê°œì„  ë²„ì „ v2")
    print("ë°ì´í„° ëˆ„ì¶œ ì œê±° + TimeSeriesSplit CV + í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print("=" * 80)

    # ê²½ë¡œ ì„¤ì •
    data_path = Path(__file__).resolve().parent / "df_refined.csv"
    output_dir = Path(__file__).resolve().parent.parent / "outputs"

    # 1. ë°ì´í„° ë¡œë“œ
    df = load_data(data_path)

    # 2. ê¸°ë³¸ Feature Engineering
    df = engineer_basic_features(df)

    # 3. ë°ì´í„° ë¶„í•  ì •ì˜
    train_years = [2023]
    val_years = [2024]
    test_years = [2025]

    # 4. ë°ì´í„° ëˆ„ì¶œ ë°©ì§€ë¥¼ ìœ„í•œ íŒ€ ì¸ê¸°ë„ ê³„ì‚°
    print("\n[ë°ì´í„° ëˆ„ì¶œ ë°©ì§€] íŒ€ ì¸ê¸°ë„ Features ê³„ì‚°")
    print("-" * 80)

    # 2023ë…„ ë°ì´í„°: 2022ë…„ ì´ì „ ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ 2023ë…„ ìƒë°˜ê¸° ë°ì´í„° ì‚¬ìš©
    # (ë˜ëŠ” ì „ë…„ë„ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒì„ ì‚¬ìš©)
    # ì—¬ê¸°ì„œëŠ” 2023ë…„ ì²« 60ê²½ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©
    print("2023ë…„ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì¸ê¸°ë„ ê³„ì‚°...")
    df_2023 = df[df['ì—°ë„'] == 2023].copy()
    train_end_2023 = df_2023.iloc[min(60, len(df_2023)-1)]['ì¼ì‹œ']
    df_2023 = calculate_popularity_features_no_leakage(df_2023, train_end_2023)
    df_2023 = calculate_moving_average_and_target(df_2023)

    # 2024ë…„ ë°ì´í„°: 2023ë…„ ì „ì²´ ë°ì´í„° ì‚¬ìš©
    print("\n2024ë…„ ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì¸ê¸°ë„ ê³„ì‚°...")
    df_2024 = df[df['ì—°ë„'] == 2024].copy()
    train_end_2024 = df[df['ì—°ë„'] == 2023]['ì¼ì‹œ'].max()
    df_2024 = calculate_popularity_features_no_leakage(df_2024, train_end_2024)
    df_2024 = calculate_moving_average_and_target(df_2024)

    # 2025ë…„ ë°ì´í„°: 2023-2024ë…„ ì „ì²´ ë°ì´í„° ì‚¬ìš©
    print("\n2025ë…„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì¸ê¸°ë„ ê³„ì‚°...")
    df_2025 = df[df['ì—°ë„'] == 2025].copy()
    train_end_2025 = df[df['ì—°ë„'].isin([2023, 2024])]['ì¼ì‹œ'].max()
    df_2025 = calculate_popularity_features_no_leakage(df_2025, train_end_2025)
    df_2025 = calculate_moving_average_and_target(df_2025)

    # ë°ì´í„° ë³‘í•©
    df = pd.concat([df_2023, df_2024, df_2025], ignore_index=True)
    df = df.sort_values('ì¼ì‹œ').reset_index(drop=True)

    print("\nâœ“ ë°ì´í„° ëˆ„ì¶œ ì—†ì´ features ìƒì„± ì™„ë£Œ!")

    # 5. Feature ì»¬ëŸ¼ ì •ì˜
    feature_columns = [
        # ì‹œê°„ Features
        'ì›”', 'ìš”ì¼_ìˆ«ì', 'ì£¼ë§ì—¬ë¶€', 'ì›”_sin', 'ì›”_cos',
        # ë‚ ì”¨ Features
        'ìµœì €ê¸°ì˜¨(Â°C)', 'ìµœê³ ê¸°ì˜¨(Â°C)', 'í‰ê· ê¸°ì˜¨',
        'ì¼ê°•ìˆ˜ëŸ‰(mm)', 'ë¹„ì—¬ë¶€', 'ê°•ìˆ˜ê°•ë„',
        'í‰ê·  ìƒëŒ€ìŠµë„(%)', 'í•©ê³„ ì¼ì‚¬ëŸ‰(MJ/m2)',
        'í‰ê·  ì „ìš´ëŸ‰(1/10)', 'ë‚ ì”¨ìƒíƒœ',
        'í‰ê·  ì§€ë©´ì˜¨ë„(Â°C)',
        # êµ¬ì¥ Features
        'êµ¬ì¥_ì¸ì½”ë”©', 'êµ¬ì¥ìˆ˜ìš©ì¸ì›', 'êµ¬ì¥í¬ê¸°',
        # íŒ€ ì¸ê¸°ë„ Features
        'í™ˆíŒ€_ì¸ì½”ë”©', 'ë°©ë¬¸íŒ€_ì¸ì½”ë”©',
        'í™ˆíŒ€ì¸ê¸°ì§€ìˆ˜', 'ë°©ë¬¸íŒ€ì¸ê¸°ì§€ìˆ˜', 'ëŒ€ì§„ì¸ê¸°ì§€ìˆ˜',
        # ì´ë™í‰ê· 
        'ì´ë™í‰ê· _30'
    ]

    # 6. ìµœì¢… NaN ì²˜ë¦¬
    df = handle_final_nans(df, feature_columns)

    # 7. TimeSeriesSplit CVë¡œ ëª¨ë¸ í•™ìŠµ
    trained_models, results_df = train_with_timeseries_cv(
        df=df,
        feature_columns=feature_columns,
        train_years=train_years,
        val_years=val_years,
        n_splits=3
    )

    # 8. ìƒìœ„ 3ê°œ ëª¨ë¸ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    print("\n[Phase 4] ìƒìœ„ ëª¨ë¸ì— ëŒ€í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print("-" * 80)

    top_3_models = results_df.head(3)['Model'].tolist()
    print(f"íŠœë‹ ëŒ€ìƒ: {top_3_models}")

    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    train_data = df[df['ì—°ë„'].isin(train_years)]
    X_train = train_data[feature_columns]
    y_train = train_data['ê´€ì¤‘ë¹„ìœ¨']

    param_grids = get_hyperparameter_grids()
    tuned_models = {}
    best_tuned_params = {}

    for model_name in top_3_models:
        if model_name in param_grids:
            base_model = trained_models[model_name]
            param_grid = param_grids[model_name]

            tuned_model, best_params = tune_hyperparameters(
                X_train=X_train,
                y_train=y_train,
                model_name=model_name,
                base_model=base_model,
                param_grid=param_grid,
                n_iter=20,
                n_splits=3
            )

            tuned_models[model_name] = tuned_model
            best_tuned_params[model_name] = best_params
        else:
            print(f"  [{model_name}] íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì—†ìŒ - ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
            tuned_models[model_name] = trained_models[model_name]

    # 9. íŠœë‹ëœ ëª¨ë¸ ì¤‘ ìµœê³  ëª¨ë¸ ì„ íƒ
    print("\n[Phase 5] íŠœë‹ëœ ëª¨ë¸ í‰ê°€")
    print("-" * 80)

    val_data = df[df['ì—°ë„'].isin(val_years)]
    X_val = val_data[feature_columns]
    y_val = val_data['ê´€ì¤‘ë¹„ìœ¨']

    tuned_results = []
    for model_name, model in tuned_models.items():
        y_val_pred = model.predict(X_val)
        val_mae = mean_absolute_error(y_val, y_val_pred)
        val_r2 = r2_score(y_val, y_val_pred)

        tuned_results.append({
            'Model': model_name,
            'Val MAE (Tuned)': val_mae,
            'Val RÂ² (Tuned)': val_r2
        })

        print(f"{model_name}: Val MAE = {val_mae:.4f}, RÂ² = {val_r2:.4f}")

    tuned_results_df = pd.DataFrame(tuned_results).sort_values('Val MAE (Tuned)')
    best_model_name = tuned_results_df.iloc[0]['Model']
    best_model = tuned_models[best_model_name]

    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (íŠœë‹ í›„): {best_model_name}")
    print(f"   Validation MAE: {tuned_results_df.iloc[0]['Val MAE (Tuned)']:.4f}")

    # 10. ìµœì¢… ëª¨ë¸ í•™ìŠµ (2023 + 2024)
    print("\n[Phase 6] ìµœì¢… ëª¨ë¸ í•™ìŠµ (2023 + 2024)")
    print("-" * 80)

    final_train_data = df[df['ì—°ë„'].isin(train_years + val_years)]
    X_final_train = final_train_data[feature_columns]
    y_final_train = final_train_data['ê´€ì¤‘ë¹„ìœ¨']

    final_model = clone(best_model)
    final_model.fit(X_final_train, y_final_train)

    print(f"ìµœì¢… í•™ìŠµ ì™„ë£Œ: {len(X_final_train)}ê²½ê¸°")

    # 11. 2025ë…„ ì˜ˆì¸¡
    print("\n[Phase 7] 2025ë…„ ì˜ˆì¸¡")
    print("-" * 80)

    test_data = df[df['ì—°ë„'].isin(test_years)].copy()

    # íŠ¸ë Œë“œ ì¶”ì •
    avg_2023 = df[df['ì—°ë„'] == 2023]['ê´€ì¤‘ìˆ˜'].mean()
    avg_2024 = df[df['ì—°ë„'] == 2024]['ê´€ì¤‘ìˆ˜'].mean()
    growth_rate = avg_2024 / avg_2023
    baseline_2025 = avg_2024 * growth_rate

    print(f"2023ë…„ í‰ê·  ê´€ì¤‘: {avg_2023:.0f}ëª…")
    print(f"2024ë…„ í‰ê·  ê´€ì¤‘: {avg_2024:.0f}ëª…")
    print(f"ì¦ê°€ìœ¨: {(growth_rate-1)*100:.1f}%")
    print(f"2025ë…„ ì¶”ì • baseline: {baseline_2025:.0f}ëª…")

    # í‰ê°€
    eval_results = evaluate_on_test(
        model=final_model,
        test_data=test_data,
        feature_columns=feature_columns,
        baseline_2025=baseline_2025
    )

    print("\n[í‰ê°€ ê²°ê³¼ - ê´€ì¤‘ ë¹„ìœ¨]")
    print(f"  MAE: {eval_results['mae_ratio']:.4f} ({eval_results['mae_ratio']*100:.1f}% ì˜¤ì°¨)")
    print(f"  RMSE: {eval_results['rmse_ratio']:.4f}")
    print(f"  RÂ²: {eval_results['r2_ratio']:.4f}")
    print(f"  MAPE: {eval_results['mape_ratio']:.2f}%")

    print("\n[í‰ê°€ ê²°ê³¼ - ê´€ì¤‘ ìˆ˜]")
    print(f"  MAE: {eval_results['mae_attendance']:.0f}ëª…")
    print(f"  RMSE: {eval_results['rmse_attendance']:.0f}ëª…")
    print(f"  RÂ²: {eval_results['r2_attendance']:.4f}")
    print(f"  MAPE: {eval_results['mape_attendance']:.2f}%")

    # 12. ì˜ˆì¸¡ ê²°ê³¼ DataFrame ìƒì„±
    test_results = test_data[['ì¼ì‹œ', 'êµ¬ì¥', 'í™ˆ', 'ë°©ë¬¸', 'ê´€ì¤‘ìˆ˜']].copy()
    test_results['ì˜ˆì¸¡ê´€ì¤‘'] = eval_results['predicted_attendance']
    test_results['ì˜¤ì°¨'] = test_results['ê´€ì¤‘ìˆ˜'] - test_results['ì˜ˆì¸¡ê´€ì¤‘']
    test_results['ì˜¤ì°¨ìœ¨(%)'] = (test_results['ì˜¤ì°¨'] / test_results['ê´€ì¤‘ìˆ˜'] * 100).abs()

    # 13. Feature Importance
    feature_importance_df = None
    if hasattr(final_model, 'feature_importances_'):
        importances = final_model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'Feature': feature_columns,
            'Importance': importances
        }).sort_values('Importance', ascending=False)

        print("\n[Top 10 ì¤‘ìš” Features]")
        print(feature_importance_df.head(10).to_string(index=False))

    # 14. ê²°ê³¼ ì €ì¥
    save_results(
        output_dir=output_dir,
        results_df=results_df,
        test_results=test_results,
        feature_importance_df=feature_importance_df,
        best_model=final_model,
        best_model_name=best_model_name,
        tuned_params=best_tuned_params.get(best_model_name)
    )

    # 15. ìµœì¢… ìš”ì•½
    print("\n" + "=" * 80)
    print("ìµœì¢… ê²°ê³¼ ìš”ì•½")
    print("=" * 80)

    print(f"\n[ëª¨ë¸ ì •ë³´]")
    print(f"  - ìµœì¢… ëª¨ë¸: {best_model_name} (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)")
    print(f"  - í•™ìŠµ ë°ì´í„°: 2023-2024ë…„ ({len(X_final_train)}ê²½ê¸°)")
    print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: 2025ë…„ ({len(test_data)}ê²½ê¸°)")
    print(f"  - Feature ìˆ˜: {len(feature_columns)}ê°œ")
    print(f"  - CV ë°©ì‹: TimeSeriesSplit (3 folds)")

    print(f"\n[ì„±ëŠ¥ ì§€í‘œ]")
    print(f"  - ê´€ì¤‘ ë¹„ìœ¨ MAE: {eval_results['mae_ratio']:.4f}")
    print(f"  - ê´€ì¤‘ ìˆ˜ MAE: {eval_results['mae_attendance']:.0f}ëª…")
    print(f"  - ê´€ì¤‘ ìˆ˜ MAPE: {eval_results['mape_attendance']:.2f}%")
    print(f"  - RÂ² Score: {eval_results['r2_attendance']:.4f}")

    if best_tuned_params.get(best_model_name):
        print(f"\n[ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°]")
        for param, value in best_tuned_params[best_model_name].items():
            print(f"  - {param}: {value}")

    print("\n" + "=" * 80)
    print("ë¶„ì„ ì™„ë£Œ!")
    print("=" * 80)

    print("\nğŸ’¡ ê°œì„  ì‚¬í•­:")
    print("  âœ… ë°ì´í„° ëˆ„ì¶œ ì™„ì „ ì œê±°")
    print("  âœ… TimeSeriesSplit êµì°¨ ê²€ì¦ ì ìš©")
    print("  âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ë£Œ")
    print("  âœ… ì½”ë“œ ëª¨ë“ˆí™” ë° êµ¬ì¡°í™”")
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. scripts/visualize_results.pyë¡œ ì‹œê°í™”")
    print("  2. outputs/ í´ë”ì—ì„œ v2 ê²°ê³¼ í™•ì¸")
    print("  3. í•„ìš”ì‹œ ì¶”ê°€ feature engineering ê³ ë ¤")


if __name__ == "__main__":
    main()
