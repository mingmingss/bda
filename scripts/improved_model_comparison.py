"""
KBO ì •ê·œì‹œì¦Œ ê´€ì¤‘ ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• - ê°œì„  ë²„ì „
3ì¡° - ëª¨ë¸ êµ¬ì¶• ë‹´ë‹¹: ì„í˜œë¦°, ìœ¤íƒœì˜

ê°œì„  ì‚¬í•­:
1. ë‹¤ì–‘í•œ ëª¨ë¸ ë¹„êµ (XGBoost, CatBoost, LightGBM, AdaBoost, ExtraTrees ì¶”ê°€)
2. Voting Ensemble ì¶”ê°€
3. ì‹œê°í™” ì½”ë“œ ì¶”ê°€
4. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ ì €ì¥
5. êµì°¨ ê²€ì¦ ê²°ê³¼ ì €ì¥
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, cross_val_score
from sklearn.ensemble import (
    RandomForestRegressor,
    HistGradientBoostingRegressor,
    AdaBoostRegressor,
    ExtraTreesRegressor,
    VotingRegressor
)
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
import warnings
from pathlib import Path
import joblib
warnings.filterwarnings('ignore')

# XGBoost, CatBoost, LightGBM import (ì„¤ì¹˜ í•„ìš”)
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸  XGBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install xgboost")

try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    print("âš ï¸  CatBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install catboost")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸  LightGBMì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install lightgbm")

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
sns.set_style('whitegrid')

print("=" * 80)
print("KBO ê´€ì¤‘ ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• í”„ë¡œì íŠ¸ - ê°œì„  ë²„ì „")
print("=" * 80)

# ====== 1. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ íƒìƒ‰ ======
print("\n[Phase 1] ë°ì´í„° ë¡œë“œ ë° íƒìƒ‰")
print("-" * 80)

data_path = Path(__file__).resolve().parent / "df_refined.csv"
df = pd.read_csv(data_path, encoding='utf-8-sig')

print(f"ë°ì´í„° shape: {df.shape}")
print(f"\nê²°ì¸¡ì¹˜ í˜„í™©:\n{df.isnull().sum()}")

# ====== 2. ë°ì´í„° ì „ì²˜ë¦¬ ======
print("\n[Phase 2] ë°ì´í„° ì „ì²˜ë¦¬")
print("-" * 80)

# 2.1 ê²°ì¸¡ì¹˜ ì²˜ë¦¬
print("2.1 ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")
missing_cols = df.columns[df.isnull().any()].tolist()
print(f"ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼: {missing_cols}")

for col in missing_cols:
    if df[col].dtype in ['float64', 'int64']:
        mean_value = df[col].mean()
        df[col].fillna(mean_value, inplace=True)
        print(f"  - {col}: í‰ê· ê°’({mean_value:.2f})ìœ¼ë¡œ ëŒ€ì²´")

# 2.2 ì¼ì‹œ ë³€í™˜
print("\n2.2 ì¼ì‹œ ë°ì´í„° ë³€í™˜...")
df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Yë…„%mì›”%dì¼')
df = df.sort_values('ì¼ì‹œ').reset_index(drop=True)
print(f"ì¼ì‹œ ë²”ìœ„: {df['ì¼ì‹œ'].min()} ~ {df['ì¼ì‹œ'].max()}")

# 2.3 ì—°ë„, ì›”, ì¼ ì¶”ì¶œ
df['ì—°ë„'] = df['ì¼ì‹œ'].dt.year
df['ì›”'] = df['ì¼ì‹œ'].dt.month
df['ì¼'] = df['ì¼ì‹œ'].dt.day
df['ì£¼ì°¨'] = df['ì¼ì‹œ'].dt.isocalendar().week

print(f"ì—°ë„ë³„ ê²½ê¸° ìˆ˜:\n{df['ì—°ë„'].value_counts().sort_index()}")

# ====== 3. Feature Engineering ======
print("\n[Phase 3] Feature Engineering")
print("-" * 80)

# 3.1 ì‹œê°„ ê´€ë ¨ Features
print("3.1 ì‹œê°„ ê´€ë ¨ Features ìƒì„±...")
weekday_map = {
    'ì›”ìš”ì¼': 0, 'í™”ìš”ì¼': 1, 'ìˆ˜ìš”ì¼': 2, 'ëª©ìš”ì¼': 3,
    'ê¸ˆìš”ì¼': 4, 'í† ìš”ì¼': 5, 'ì¼ìš”ì¼': 6
}
df['ìš”ì¼_ìˆ«ì'] = df['ìš”ì¼'].map(weekday_map)
df['ì£¼ë§ì—¬ë¶€'] = df['ìš”ì¼_ìˆ«ì'].apply(lambda x: 1 if x >= 4 else 0)
df['ì›”_sin'] = np.sin(2 * np.pi * df['ì›”'] / 12)
df['ì›”_cos'] = np.cos(2 * np.pi * df['ì›”'] / 12)

# 3.2 ë‚ ì”¨ ê´€ë ¨ Features
print("\n3.2 ë‚ ì”¨ ê´€ë ¨ Features ìƒì„±...")
df['í‰ê· ê¸°ì˜¨'] = (df['ìµœì €ê¸°ì˜¨(Â°C)'] + df['ìµœê³ ê¸°ì˜¨(Â°C)']) / 2
df['ë¹„ì—¬ë¶€'] = (df['ì¼ê°•ìˆ˜ëŸ‰(mm)'] > 0).astype(int)

def categorize_rain(rain):
    if rain == 0:
        return 0
    elif rain < 5:
        return 1
    elif rain < 20:
        return 2
    else:
        return 3

df['ê°•ìˆ˜ê°•ë„'] = df['ì¼ê°•ìˆ˜ëŸ‰(mm)'].apply(categorize_rain)

def categorize_weather(cloud):
    if cloud <= 3:
        return 0
    elif cloud <= 7:
        return 1
    else:
        return 2

df['ë‚ ì”¨ìƒíƒœ'] = df['í‰ê·  ì „ìš´ëŸ‰(1/10)'].apply(categorize_weather)

# 3.3 êµ¬ì¥ ê´€ë ¨ Features
print("\n3.3 êµ¬ì¥ ê´€ë ¨ Features ìƒì„±...")
stadium_capacity = {
    'ì ì‹¤': 25000, 'ìˆ˜ì›': 20000, 'ë¬¸í•™': 20500, 'ê³ ì²™': 16500,
    'ê´‘ì£¼': 11000, 'ì‚¬ì§': 23000, 'ì°½ì›': 20000, 'ëŒ€êµ¬': 10000,
    'í•œë°­': 11500, 'ëŒ€ì „': 13000, 'ìš¸ì‚°': 11000, 'í¬í•­': 11000, 'ì²­ì£¼': 10000
}

df['êµ¬ì¥ìˆ˜ìš©ì¸ì›'] = df['êµ¬ì¥'].map(stadium_capacity)
df['êµ¬ì¥í¬ê¸°'] = pd.cut(df['êµ¬ì¥ìˆ˜ìš©ì¸ì›'],
                       bins=[0, 12000, 20000, 30000],
                       labels=[0, 1, 2]).astype(float)

# 3.4 íŒ€ ì¸ê¸°ë„ Features
print("\n3.4 íŒ€ ì¸ê¸°ë„ Features ìƒì„±...")
train_years = df[df['ì—°ë„'].isin([2023, 2024])]
overall_avg = train_years['ê´€ì¤‘ìˆ˜'].mean()

home_popularity = train_years.groupby('í™ˆ')['ê´€ì¤‘ìˆ˜'].mean().to_dict()
df['í™ˆíŒ€í‰ê· ê´€ì¤‘'] = df['í™ˆ'].map(home_popularity)
df['í™ˆíŒ€í‰ê· ê´€ì¤‘'].fillna(overall_avg, inplace=True)

away_popularity = train_years.groupby('ë°©ë¬¸')['ê´€ì¤‘ìˆ˜'].mean().to_dict()
df['ë°©ë¬¸íŒ€í‰ê· ê´€ì¤‘'] = df['ë°©ë¬¸'].map(away_popularity)
df['ë°©ë¬¸íŒ€í‰ê· ê´€ì¤‘'].fillna(overall_avg, inplace=True)

df['í™ˆíŒ€ì¸ê¸°ì§€ìˆ˜'] = df['í™ˆíŒ€í‰ê· ê´€ì¤‘'] / overall_avg
df['ë°©ë¬¸íŒ€ì¸ê¸°ì§€ìˆ˜'] = df['ë°©ë¬¸íŒ€í‰ê· ê´€ì¤‘'] / overall_avg

matchup = train_years.groupby(['í™ˆ', 'ë°©ë¬¸'])['ê´€ì¤‘ìˆ˜'].mean().to_dict()
df['ëŒ€ì§„í‰ê· ê´€ì¤‘'] = df.apply(lambda row: matchup.get((row['í™ˆ'], row['ë°©ë¬¸']), overall_avg), axis=1)
df['ëŒ€ì§„ì¸ê¸°ì§€ìˆ˜'] = df['ëŒ€ì§„í‰ê· ê´€ì¤‘'] / overall_avg

# 3.5 ì´ë™í‰ê·  ë° ëª©í‘œ ë³€ìˆ˜ ê³„ì‚°
print("\n3.5 ëª©í‘œ ë³€ìˆ˜(ê´€ì¤‘ ë¹„ìœ¨) ê³„ì‚°...")
df['ì´ë™í‰ê· _30'] = df['ê´€ì¤‘ìˆ˜'].rolling(window=30, min_periods=1).mean().shift(1)
initial_avg = df.iloc[:30]['ê´€ì¤‘ìˆ˜'].mean()
df.loc[df['ì´ë™í‰ê· _30'].isna(), 'ì´ë™í‰ê· _30'] = initial_avg
df['ê´€ì¤‘ë¹„ìœ¨'] = df['ê´€ì¤‘ìˆ˜'] / df['ì´ë™í‰ê· _30']
df['ë§¤ì§„ìœ¨'] = df['ê´€ì¤‘ìˆ˜'] / df['êµ¬ì¥ìˆ˜ìš©ì¸ì›']

print(f"  - ê´€ì¤‘ë¹„ìœ¨ í‰ê· : {df['ê´€ì¤‘ë¹„ìœ¨'].mean():.3f}")
print(f"  - ê´€ì¤‘ë¹„ìœ¨ ë²”ìœ„: {df['ê´€ì¤‘ë¹„ìœ¨'].min():.3f} ~ {df['ê´€ì¤‘ë¹„ìœ¨'].max():.3f}")

# 3.6 ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
print("\n3.6 ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©...")
le_stadium = LabelEncoder()
le_home = LabelEncoder()
le_away = LabelEncoder()

df['êµ¬ì¥_ì¸ì½”ë”©'] = le_stadium.fit_transform(df['êµ¬ì¥'])
df['í™ˆíŒ€_ì¸ì½”ë”©'] = le_home.fit_transform(df['í™ˆ'])
df['ë°©ë¬¸íŒ€_ì¸ì½”ë”©'] = le_away.fit_transform(df['ë°©ë¬¸'])

# 3.7 ìµœì¢… NaN ì²´í¬ ë° ì²˜ë¦¬
print("\n3.7 ìµœì¢… NaN ì²´í¬ ë° ì²˜ë¦¬...")
nan_counts = df.isnull().sum()
nan_columns = nan_counts[nan_counts > 0]

if len(nan_columns) > 0:
    print(f"âš ï¸  Feature Engineering í›„ NaN ë°œê²¬:")
    for col in nan_columns.index:
        print(f"  - {col}: {nan_columns[col]}ê°œ")
        if df[col].dtype in ['float64', 'int64']:
            # ìˆ˜ì¹˜í˜•: í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
            mean_val = df[col].mean()
            if pd.isna(mean_val):  # í‰ê· ë„ NaNì´ë©´ 0ìœ¼ë¡œ ëŒ€ì²´
                df[col].fillna(0, inplace=True)
                print(f"    â†’ 0ìœ¼ë¡œ ëŒ€ì²´")
            else:
                df[col].fillna(mean_val, inplace=True)
                print(f"    â†’ í‰ê· ê°’({mean_val:.2f})ìœ¼ë¡œ ëŒ€ì²´")
        else:
            # ë²”ì£¼í˜•: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
            mode_val = df[col].mode()[0] if len(df[col].mode()) > 0 else 0
            df[col].fillna(mode_val, inplace=True)
            print(f"    â†’ ìµœë¹ˆê°’({mode_val})ìœ¼ë¡œ ëŒ€ì²´")
else:
    print("âœ“ NaN ì—†ìŒ - ëª¨ë“  Featureê°€ ì •ìƒì…ë‹ˆë‹¤!")

# ====== 4. ë°ì´í„° ë¶„í•  ======
print("\n[Phase 4] ë°ì´í„° ë¶„í• ")
print("-" * 80)

train_data = df[df['ì—°ë„'] == 2023].copy()
val_data = df[df['ì—°ë„'] == 2024].copy()
test_data = df[df['ì—°ë„'] == 2025].copy()

print(f"Train (2023): {len(train_data)}ê²½ê¸°")
print(f"Validation (2024): {len(val_data)}ê²½ê¸°")
print(f"Test (2025): {len(test_data)}ê²½ê¸°")

# Feature ì„ íƒ
feature_columns = [
    # ì‹œê°„ Features
    'ì›”', 'ìš”ì¼_ìˆ«ì', 'ì£¼ë§ì—¬ë¶€', 'ì›”_sin', 'ì›”_cos',
    # ë‚ ì”¨ Features
    'ìµœì €ê¸°ì˜¨(Â°C)', 'ìµœê³ ê¸°ì˜¨(Â°C)', 'í‰ê· ê¸°ì˜¨',
    'ì¼ê°•ìˆ˜ëŸ‰(mm)', 'ë¹„ì—¬ë¶€', 'ê°•ìˆ˜ê°•ë„',
    'í‰ê·  ìƒëŒ€ìŠµë„(%)', 'í•©ê³„ ì¼ì‚¬ëŸ‰(MJ/m2)',
    'í‰ê·  ì „ìš´ëŸ‰(1/10)', 'ë‚ ì”¨ìƒíƒœ',
    'í‰ê·  ì§€ë©´ì˜¨ë„(Â°C)',
    # êµ¬ì¥ Features
    'êµ¬ì¥_ì¸ì½”ë”©', 'êµ¬ì¥ìˆ˜ìš©ì¸ì›', 'êµ¬ì¥í¬ê¸°',
    # íŒ€ ì¸ê¸°ë„ Features
    'í™ˆíŒ€_ì¸ì½”ë”©', 'ë°©ë¬¸íŒ€_ì¸ì½”ë”©',
    'í™ˆíŒ€ì¸ê¸°ì§€ìˆ˜', 'ë°©ë¬¸íŒ€ì¸ê¸°ì§€ìˆ˜', 'ëŒ€ì§„ì¸ê¸°ì§€ìˆ˜',
    # ì´ë™í‰ê· 
    'ì´ë™í‰ê· _30'
]

print(f"\nì‚¬ìš©í•  Feature ìˆ˜: {len(feature_columns)}ê°œ")

X_train = train_data[feature_columns]
y_train = train_data['ê´€ì¤‘ë¹„ìœ¨']

X_val = val_data[feature_columns]
y_val = val_data['ê´€ì¤‘ë¹„ìœ¨']

X_test = test_data[feature_columns]
y_test = test_data['ê´€ì¤‘ë¹„ìœ¨']

print(f"\nX_train shape: {X_train.shape}")
print(f"X_val shape: {X_val.shape}")
print(f"X_test shape: {X_test.shape}")

# ====== 5. ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ ======
print("\n[Phase 5] ë‹¤ì–‘í•œ ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ")
print("-" * 80)

# ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
models = {}

# 5.1 Decision Tree
print("\n5.1 Decision Tree Regressor í•™ìŠµ...")
models['Decision Tree'] = DecisionTreeRegressor(random_state=42, max_depth=10)

# 5.2 Random Forest
print("5.2 Random Forest Regressor í•™ìŠµ...")
models['Random Forest'] = RandomForestRegressor(
    n_estimators=200,
    max_depth=15,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42,
    n_jobs=-1
)

# 5.3 Extra Trees
print("5.3 Extra Trees Regressor í•™ìŠµ...")
models['Extra Trees'] = ExtraTreesRegressor(
    n_estimators=200,
    max_depth=15,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42,
    n_jobs=-1
)

# 5.4 Hist Gradient Boosting
print("5.4 Hist Gradient Boosting Regressor í•™ìŠµ...")
models['Hist Gradient Boosting'] = HistGradientBoostingRegressor(
    max_iter=200,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)

# 5.5 AdaBoost
print("5.5 AdaBoost Regressor í•™ìŠµ...")
models['AdaBoost'] = AdaBoostRegressor(
    estimator=DecisionTreeRegressor(max_depth=5),
    n_estimators=100,
    learning_rate=0.1,
    random_state=42
)

# 5.6 XGBoost (if available)
if XGBOOST_AVAILABLE:
    print("5.6 XGBoost Regressor í•™ìŠµ...")
    models['XGBoost'] = xgb.XGBRegressor(
        n_estimators=200,
        max_depth=5,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    )

# 5.7 CatBoost (if available)
if CATBOOST_AVAILABLE:
    print("5.7 CatBoost Regressor í•™ìŠµ...")
    models['CatBoost'] = cb.CatBoostRegressor(
        iterations=200,
        depth=5,
        learning_rate=0.1,
        random_state=42,
        verbose=0
    )

# 5.8 LightGBM (if available)
if LIGHTGBM_AVAILABLE:
    print("5.8 LightGBM Regressor í•™ìŠµ...")
    models['LightGBM'] = lgb.LGBMRegressor(
        n_estimators=200,
        max_depth=5,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        verbose=-1
    )

# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
print("\nëª¨ë¸ í•™ìŠµ ì¤‘...")
print("=" * 80)

results = []

for name, model in models.items():
    print(f"\n[{name}] í•™ìŠµ ì¤‘...")

    # í•™ìŠµ
    model.fit(X_train, y_train)

    # ì˜ˆì¸¡
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)

    # í‰ê°€
    train_mae = mean_absolute_error(y_train, y_train_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    train_r2 = r2_score(y_train, y_train_pred)

    val_mae = mean_absolute_error(y_val, y_val_pred)
    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    val_r2 = r2_score(y_val, y_val_pred)

    # ê²°ê³¼ ì €ì¥
    results.append({
        'Model': name,
        'Train MAE': train_mae,
        'Train RMSE': train_rmse,
        'Train RÂ²': train_r2,
        'Val MAE': val_mae,
        'Val RMSE': val_rmse,
        'Val RÂ²': val_r2,
        'Overfit Score': train_mae - val_mae  # ìŒìˆ˜ë©´ ì¢‹ìŒ
    })

    print(f"  Train - MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}, RÂ²: {train_r2:.4f}")
    print(f"  Val   - MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, RÂ²: {val_r2:.4f}")
    print(f"  Overfit: {train_mae - val_mae:.4f} (ìŒìˆ˜ê°€ ì¢‹ìŒ)")

# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('Val MAE')

print("\n" + "=" * 80)
print("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (Validation MAE ê¸°ì¤€ ì •ë ¬)")
print("=" * 80)
print(results_df.to_string(index=False))

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
best_model_name = results_df.iloc[0]['Model']
best_model = models[best_model_name]
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
print(f"   Validation MAE: {results_df.iloc[0]['Val MAE']:.4f}")

# ====== 6. Voting Ensemble ======
print("\n[Phase 6] Voting Ensemble êµ¬ì¶•")
print("-" * 80)

# ìƒìœ„ 3ê°œ ëª¨ë¸ë¡œ Voting Ensemble êµ¬ì„±
top_3_models = results_df.head(3)['Model'].tolist()
print(f"ìƒìœ„ 3ê°œ ëª¨ë¸: {top_3_models}")

ensemble_estimators = [(name, models[name]) for name in top_3_models]
voting_model = VotingRegressor(estimators=ensemble_estimators)

print("Voting Ensemble í•™ìŠµ ì¤‘...")
voting_model.fit(X_train, y_train)

y_val_pred_voting = voting_model.predict(X_val)
val_mae_voting = mean_absolute_error(y_val, y_val_pred_voting)
val_rmse_voting = np.sqrt(mean_squared_error(y_val, y_val_pred_voting))
val_r2_voting = r2_score(y_val, y_val_pred_voting)

print(f"\nVoting Ensemble ì„±ëŠ¥:")
print(f"  Val MAE: {val_mae_voting:.4f}")
print(f"  Val RMSE: {val_rmse_voting:.4f}")
print(f"  Val RÂ²: {val_r2_voting:.4f}")

# Voting Ensembleì´ ë” ì¢‹ìœ¼ë©´ ìµœê³  ëª¨ë¸ë¡œ ì„ íƒ
if val_mae_voting < results_df.iloc[0]['Val MAE']:
    print("\nâœ“ Voting Ensembleì´ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•©ë‹ˆë‹¤!")
    best_model = voting_model
    best_model_name = 'Voting Ensemble'
else:
    print(f"\nâœ“ {best_model_name}ì´ Voting Ensembleë³´ë‹¤ ìš°ìˆ˜í•©ë‹ˆë‹¤.")

# ====== 7. ìµœì¢… ëª¨ë¸ í•™ìŠµ (2023 + 2024) ======
print("\n[Phase 7] ìµœì¢… ëª¨ë¸ í•™ìŠµ (2023 + 2024 ë°ì´í„°)")
print("-" * 80)

X_final_train = pd.concat([X_train, X_val])
y_final_train = pd.concat([y_train, y_val])

print(f"ìµœì¢… í•™ìŠµ ë°ì´í„°: {X_final_train.shape}")
print(f"ìµœì¢… ëª¨ë¸: {best_model_name}")

# ìµœì¢… ëª¨ë¸ ì¬í•™ìŠµ
if best_model_name == 'Voting Ensemble':
    # Voting Ensembleì€ ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì¬í•™ìŠµ í•„ìš”
    ensemble_estimators_final = []
    for name in top_3_models:
        # ê° ëª¨ë¸ì„ 2023+2024 ë°ì´í„°ë¡œ ì¬í•™ìŠµ
        model_copy = models[name]
        model_copy.fit(X_final_train, y_final_train)
        ensemble_estimators_final.append((name, model_copy))

    final_model = VotingRegressor(estimators=ensemble_estimators_final)
    final_model.fit(X_final_train, y_final_train)
else:
    # ë‹¨ì¼ ëª¨ë¸ì¸ ê²½ìš°
    final_model = best_model
    final_model.fit(X_final_train, y_final_train)

print("ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

# ====== 8. 2025ë…„ ì˜ˆì¸¡ ======
print("\n[Phase 8] 2025ë…„ ì˜ˆì¸¡")
print("-" * 80)

# 8.1 íŠ¸ë Œë“œ ì¶”ì •
print("8.1 2025ë…„ íŠ¸ë Œë“œ ì¶”ì •...")
avg_2023 = train_data['ê´€ì¤‘ìˆ˜'].mean()
avg_2024 = val_data['ê´€ì¤‘ìˆ˜'].mean()
growth_rate = avg_2024 / avg_2023

print(f"2023ë…„ í‰ê·  ê´€ì¤‘: {avg_2023:.0f}ëª…")
print(f"2024ë…„ í‰ê·  ê´€ì¤‘: {avg_2024:.0f}ëª…")
print(f"ì¦ê°€ìœ¨: {(growth_rate-1)*100:.1f}%")

baseline_2025_estimated = avg_2024 * growth_rate
baseline_2025_actual = test_data['ì´ë™í‰ê· _30'].mean()

print(f"2025ë…„ ì¶”ì • ê¸°ë³¸ê°’: {baseline_2025_estimated:.0f}ëª…")
print(f"2025ë…„ ì‹¤ì œ ê¸°ë³¸ê°’: {baseline_2025_actual:.0f}ëª…")

# 8.2 ë¹„ìœ¨ ì˜ˆì¸¡
print("\n8.2 ê´€ì¤‘ ë¹„ìœ¨ ì˜ˆì¸¡...")
y_test_pred_ratio = final_model.predict(X_test)

print(f"ì˜ˆì¸¡ ë¹„ìœ¨ ë²”ìœ„: {y_test_pred_ratio.min():.3f} ~ {y_test_pred_ratio.max():.3f}")
print(f"ì˜ˆì¸¡ ë¹„ìœ¨ í‰ê· : {y_test_pred_ratio.mean():.3f}")

# 8.3 ì‹¤ì œ ê´€ì¤‘ ìˆ˜ ë³€í™˜
print("\n8.3 ì‹¤ì œ ê´€ì¤‘ ìˆ˜ ë³€í™˜...")
test_data['ì˜ˆì¸¡ê´€ì¤‘_ì¶”ì •íŠ¸ë Œë“œ'] = y_test_pred_ratio * baseline_2025_estimated
test_data['ì˜ˆì¸¡ê´€ì¤‘_ì‹¤ì œíŠ¸ë Œë“œ'] = y_test_pred_ratio * test_data['ì´ë™í‰ê· _30']

print(f"ì¶”ì • íŠ¸ë Œë“œ ê¸°ë°˜ ì˜ˆì¸¡ í‰ê· : {test_data['ì˜ˆì¸¡ê´€ì¤‘_ì¶”ì •íŠ¸ë Œë“œ'].mean():.0f}ëª…")
print(f"ì‹¤ì œ íŠ¸ë Œë“œ ê¸°ë°˜ ì˜ˆì¸¡ í‰ê· : {test_data['ì˜ˆì¸¡ê´€ì¤‘_ì‹¤ì œíŠ¸ë Œë“œ'].mean():.0f}ëª…")
print(f"ì‹¤ì œ ê´€ì¤‘ í‰ê· : {test_data['ê´€ì¤‘ìˆ˜'].mean():.0f}ëª…")

# ====== 9. í‰ê°€ ë° ì˜¤ì°¨ ë¶„ì„ ======
print("\n[Phase 9] í‰ê°€ ë° ì˜¤ì°¨ ë¶„ì„")
print("-" * 80)

# 9.1 ë¹„ìœ¨ ê¸°ì¤€ í‰ê°€
mae_ratio = mean_absolute_error(y_test, y_test_pred_ratio)
rmse_ratio = np.sqrt(mean_squared_error(y_test, y_test_pred_ratio))
r2_ratio = r2_score(y_test, y_test_pred_ratio)
mape_ratio = np.mean(np.abs((y_test - y_test_pred_ratio) / y_test)) * 100

print("\n9.1 ë¹„ìœ¨ ê¸°ì¤€ í‰ê°€")
print(f"MAE (ë¹„ìœ¨): {mae_ratio:.4f} (í‰ê·  {mae_ratio*100:.1f}% ì˜¤ì°¨)")
print(f"RMSE (ë¹„ìœ¨): {rmse_ratio:.4f}")
print(f"RÂ² Score: {r2_ratio:.4f}")
print(f"MAPE: {mape_ratio:.2f}%")

# 9.2 ê´€ì¤‘ ìˆ˜ ê¸°ì¤€ í‰ê°€
A = test_data['ê´€ì¤‘ìˆ˜'].values
B = test_data['ì˜ˆì¸¡ê´€ì¤‘_ì¶”ì •íŠ¸ë Œë“œ'].values
C = test_data['ì˜ˆì¸¡ê´€ì¤‘_ì‹¤ì œíŠ¸ë Œë“œ'].values

mae_attendance = mean_absolute_error(A, B)
rmse_attendance = np.sqrt(mean_squared_error(A, B))
r2_attendance = r2_score(A, B)
mape_attendance = np.mean(np.abs((A - B) / A)) * 100

print("\n9.2 ê´€ì¤‘ ìˆ˜ ê¸°ì¤€ í‰ê°€")
print(f"MAE (ê´€ì¤‘ìˆ˜): {mae_attendance:.0f}ëª…")
print(f"RMSE (ê´€ì¤‘ìˆ˜): {rmse_attendance:.0f}ëª…")
print(f"RÂ² Score: {r2_attendance:.4f}")
print(f"MAPE: {mape_attendance:.2f}%")

# 9.3 ì˜¤ì°¨ ë¶„í•´ ë¶„ì„
total_error = A - B
trend_error = C - B
model_error = A - C

mae_total = np.mean(np.abs(total_error))
mae_trend = np.mean(np.abs(trend_error))
mae_model = np.mean(np.abs(model_error))

print("\n9.3 ì˜¤ì°¨ ë¶„í•´ ë¶„ì„")
print(f"ì´ ì˜¤ì°¨ (MAE): {mae_total:.0f}ëª…")
print(f"  â”œâ”€ íŠ¸ë Œë“œ ì¶”ì • ì˜¤ì°¨: {mae_trend:.0f}ëª… ({mae_trend/mae_total*100:.1f}%)")
print(f"  â””â”€ ëª¨ë¸ íŒ¨í„´ ì˜¤ì°¨: {mae_model:.0f}ëª… ({mae_model/mae_total*100:.1f}%)")

# ====== 10. Feature Importance (ê°€ëŠ¥í•œ ê²½ìš°) ======
print("\n[Phase 10] Feature Importance ë¶„ì„")
print("-" * 80)

if hasattr(final_model, 'feature_importances_'):
    importances = final_model.feature_importances_
    feature_importance_df = pd.DataFrame({
        'Feature': feature_columns,
        'Importance': importances
    }).sort_values('Importance', ascending=False)

    print("\nTop 15 ì¤‘ìš” Features:")
    print(feature_importance_df.head(15).to_string(index=False))
else:
    print("í•´ë‹¹ ëª¨ë¸ì€ Feature Importanceë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    feature_importance_df = None

# ====== 11. ê²°ê³¼ ì €ì¥ ======
print("\n[Phase 11] ê²°ê³¼ ì €ì¥")
print("-" * 80)

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
output_dir = Path(__file__).resolve().parent.parent / "outputs"
output_dir.mkdir(exist_ok=True)

# 11.1 ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ ì €ì¥
results_df.to_csv(output_dir / "model_performance_comparison.csv", index=False, encoding='utf-8-sig')
print(f"âœ“ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ ì €ì¥: {output_dir / 'model_performance_comparison.csv'}")

# 11.2 ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
prediction_df = test_data[['ì¼ì‹œ', 'êµ¬ì¥', 'í™ˆ', 'ë°©ë¬¸', 'ê´€ì¤‘ìˆ˜', 'ì˜ˆì¸¡ê´€ì¤‘_ì¶”ì •íŠ¸ë Œë“œ', 'ì˜ˆì¸¡ê´€ì¤‘_ì‹¤ì œíŠ¸ë Œë“œ']].copy()
prediction_df['ì˜¤ì°¨'] = prediction_df['ê´€ì¤‘ìˆ˜'] - prediction_df['ì˜ˆì¸¡ê´€ì¤‘_ì¶”ì •íŠ¸ë Œë“œ']
prediction_df.to_csv(output_dir / "2025_predictions_detailed.csv", index=False, encoding='utf-8-sig')
print(f"âœ“ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {output_dir / '2025_predictions_detailed.csv'}")

# 11.3 Feature Importance ì €ì¥
if feature_importance_df is not None:
    feature_importance_df.to_csv(output_dir / "feature_importance.csv", index=False, encoding='utf-8-sig')
    print(f"âœ“ Feature Importance ì €ì¥: {output_dir / 'feature_importance.csv'}")

# 11.4 ìµœì¢… ëª¨ë¸ ì €ì¥
model_path = output_dir / f"best_model_{best_model_name.replace(' ', '_')}.pkl"
joblib.dump(final_model, model_path)
print(f"âœ“ ìµœì¢… ëª¨ë¸ ì €ì¥: {model_path}")

# ====== 12. ìµœì¢… ìš”ì•½ ======
print("\n" + "=" * 80)
print("ìµœì¢… ê²°ê³¼ ìš”ì•½")
print("=" * 80)

print(f"\n[ëª¨ë¸ ì •ë³´]")
print(f"  - ìµœì¢… ëª¨ë¸: {best_model_name}")
print(f"  - í•™ìŠµ ë°ì´í„°: 2023-2024ë…„ ({len(X_final_train)}ê²½ê¸°)")
print(f"  - í…ŒìŠ¤íŠ¸ ë°ì´í„°: 2025ë…„ ({len(X_test)}ê²½ê¸°)")
print(f"  - Feature ìˆ˜: {len(feature_columns)}ê°œ")

print(f"\n[ì„±ëŠ¥ ì§€í‘œ - ê´€ì¤‘ ë¹„ìœ¨]")
print(f"  - MAE: {mae_ratio:.4f} ({mae_ratio*100:.1f}% ì˜¤ì°¨)")
print(f"  - RMSE: {rmse_ratio:.4f}")
print(f"  - RÂ²: {r2_ratio:.4f}")
print(f"  - MAPE: {mape_ratio:.2f}%")

print(f"\n[ì„±ëŠ¥ ì§€í‘œ - ê´€ì¤‘ ìˆ˜]")
print(f"  - MAE: {mae_attendance:.0f}ëª…")
print(f"  - RMSE: {rmse_attendance:.0f}ëª…")
print(f"  - RÂ²: {r2_attendance:.4f}")
print(f"  - MAPE: {mape_attendance:.2f}%")

print(f"\n[ì˜¤ì°¨ ë¶„ì„]")
print(f"  - ì´ ì˜¤ì°¨: {mae_total:.0f}ëª…")
print(f"    â”œâ”€ íŠ¸ë Œë“œ ì¶”ì • ì˜¤ì°¨: {mae_trend:.0f}ëª… ({mae_trend/mae_total*100:.1f}%)")
print(f"    â””â”€ ëª¨ë¸ íŒ¨í„´ ì˜¤ì°¨: {mae_model:.0f}ëª… ({mae_model/mae_total*100:.1f}%)")

if feature_importance_df is not None:
    print(f"\n[Top 5 ì¤‘ìš” Features]")
    for idx, row in feature_importance_df.head(5).iterrows():
        print(f"  {idx+1}. {row['Feature']}: {row['Importance']:.4f}")

print("\n[ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„]")
for idx, row in results_df.head(5).iterrows():
    print(f"  {idx+1}. {row['Model']}: Val MAE = {row['Val MAE']:.4f}")

print("\n" + "=" * 80)
print("ë¶„ì„ ì™„ë£Œ!")
print("=" * 80)

print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
print("  1. scripts/visualize_results.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ì‹œê°í™” ìƒì„±")
print("  2. outputs/ í´ë”ì—ì„œ ê²°ê³¼ í™•ì¸")
print("  3. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê³ ë ¤")
